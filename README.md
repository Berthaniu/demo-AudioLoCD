## AudioLoCD: An Efficient Conditional Discrete Contrastive Latent Diffusion Model for Text-to-Audio Generation

### About 

This is a [demo](https://Berthaniu.github.io/demo-AudioLoCD/) for our [paper](https://Berthaniu.github.io/demo-AudioLoCD/) '_AudioLoCD: An Efficient Conditional Discrete Contrastive Latent Diffusion Model for Text-to-Audio Generation_'. The AudioLoCD is a LoRA-based conditional discrete contrastive latent diffusion model for text-to-audio generation. Unlike recent large-scale TTA models, our AudioLoCD can be efficiently trained under limited computational resources. The integration of a contrastive learning strategy enhances the connection between textual conditions and the generated audio outputs, resulting in coherent performance. 

### Citation

If you are interesting in our work, please cite it as below:

```
@INPROCEEDINGS{niu2023audiolocd,
  author={Niu, Xinlei, Jing Zhang, Christian Walder, and Charles Patrick Martin}
  title={AudioLoCD: An Efficient Conditional Discrete Contrastive Latent Diffusion Model for Text-to-Audio Generation}, 
  year={2023}
}
```